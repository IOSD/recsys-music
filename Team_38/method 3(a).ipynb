{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "I have used the movies dataset(100K) since the song dataset(1000K) which I used in method 1,method 2 was quite large\n",
    "This method uses the keras library with neural network\n",
    "We will try to make features of user and item using known rating\n",
    "We will then merge the features of user and item and fed it to neural network\n",
    "We will get the output of desired rating from neural network\n",
    "We will then use these learnt features to predict new rating\n",
    "'''\n",
    "#Importing necessary liraries\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.constraints import non_neg\n",
    "import numpy as np\n",
    "import keras.models as kmodels\n",
    "import keras.layers as klayers\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import concatenate\n"  
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Size of our full dataset###\n",
      "\n",
      "(100000, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Collecting the necessary movielens data\n",
    "data=pd.read_csv(r\"C:\\Users\\shashank\\Desktop\\music recommendation\\ml-100k\\u.data\" \\\n",
    ",sep='\\t',names=\"user_id,item_id,rating,timestamp\".split(\",\"))\n",
    "\n",
    "'''\n",
    "data.user_id = data.user_id.astype('category').cat.codes.values\n",
    "data.item_id = data.item_id.astype('category').cat.codes.values\n",
    "'''\n",
    "print('###Size of our full dataset###')\n",
    "print()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###Size of our training dataset\n",
      "(80000, 4)\n",
      "\n",
      "###Size of our test dataset\n",
      "(20000, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#all_user contains id of all users\n",
    "all_user = data['user_id'].unique().tolist()\n",
    "#all_item contains id of all items\n",
    "all_item = data['item_id'].unique().tolist()\n",
    "\n",
    "#dict1(dictionary) contains modified ids of users between 0 and len(all_user)\n",
    "dict1 = {j:i for i,j in enumerate(all_user)} \n",
    "#dict2(dictionary) contains modified ids of users between 0 and len(all_item)\n",
    "dict2 = {j:i for i,j in enumerate(all_item)} \n",
    "\n",
    "#below for loop will convert old ids to new ids present in dict1 and dict2\n",
    "for item,row in data.iterrows():\n",
    "\trow['user_id'] = dict1[row['user_id']]\n",
    "\trow['item_id'] = dict2[row['item_id']]\n",
    "\n",
    "#Splitting of data\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "print()\n",
    "print('###Size of our training dataset')\n",
    "print(train.shape)\n",
    "print()\n",
    "print('###Size of our test dataset')\n",
    "print(test.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#users contain all unique user id\n",
    "users = len(data.user_id.unique())\n",
    "\n",
    "#movies contain all unique movie id\n",
    "movies = len(data.item_id.unique()) \n",
    "print('')\n",
    "\n",
    "#total features we would like to learn for user and movie\n",
    "features = 30\n",
    "\n",
    "movie = keras.layers.Input(shape=[1],name='movie_input')\n",
    "\n",
    "#this creates a feature space for item\n",
    "\n",
    "feature_space_movie = keras.layers.Embedding(movies , features, name='movie_feature_space',embeddings_constraint=non_neg())(movie)\n",
    "movie_flattened = keras.layers.Flatten()(feature_space_movie)\n",
    "\n",
    "user = keras.layers.Input(shape=[1],name='user_input')\n",
    "\n",
    "#this creates feature space for user\n",
    "feature_space_user = keras.layers.Embedding(users, features,name='User_Feature_space',embeddings_constraint=non_neg())(user)\n",
    "user_flattened = keras.layers.Flatten()(feature_space_user)\n",
    "\n",
    "#lets merge features of user and movie to fed to neural network\n",
    "\n",
    "model = concatenate([movie_flattened,user_flattened],axis=-1)\n",
    "\n",
    "#fedding to neural network\n",
    "\n",
    "model = keras.layers.Dense(45,activation='relu')(model)\n",
    "\n",
    "model = keras.layers.Dropout(0.35)(model)\n",
    "\n",
    "model = keras.layers.Dense(25,activation='relu')(model)\n",
    "\n",
    "model = keras.layers.Dropout(0.25)(model)\n",
    "\n",
    "model = keras.layers.Dense(1)(model)\n",
    "\n",
    "result = keras.Model(inputs=[user, movie],outputs=[model])\n",
    "\n",
    "result.compile('adam', 'mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "movie_input (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_feature_space (Embedding) (None, 1, 30)        50460       movie_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "User_Feature_space (Embedding)  (None, 1, 30)        28290       user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 30)           0           movie_feature_space[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 30)           0           User_Feature_space[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 60)           0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 45)           2745        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 45)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 25)           1150        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 25)           0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            26          dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 82,671\n",
      "Trainable params: 82,671\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80000/80000 [==============================] - 19s 239us/step - loss: 1.6480\n",
      "Epoch 2/10\n",
      "80000/80000 [==============================] - 19s 240us/step - loss: 1.1084\n",
      "Epoch 3/10\n",
      "80000/80000 [==============================] - 17s 216us/step - loss: 1.0049\n",
      "Epoch 4/10\n",
      "80000/80000 [==============================] - 17s 209us/step - loss: 0.9470\n",
      "Epoch 5/10\n",
      "80000/80000 [==============================] - 17s 217us/step - loss: 0.9092\n",
      "Epoch 6/10\n",
      "80000/80000 [==============================] - 17s 215us/step - loss: 0.8824\n",
      "Epoch 7/10\n",
      "80000/80000 [==============================] - 17s 215us/step - loss: 0.8649\n",
      "Epoch 8/10\n",
      "80000/80000 [==============================] - 17s 216us/step - loss: 0.8502\n",
      "Epoch 9/10\n",
      "80000/80000 [==============================] - 17s 217us/step - loss: 0.8371\n",
      "Epoch 10/10\n",
      "80000/80000 [==============================] - 19s 243us/step - loss: 0.8234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b7be85a470>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will make 10 iterations through full train dataset\n",
    "result.fit([train.user_id, train.item_id], train.rating, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###The error in Training set is: 0.6721769634380936\n",
      "\n",
      "###The error in Test set is: 0.7284379516422749\n"
     ]
    }
   ],
   "source": [
    "predicted = result.predict([train.user_id, train.item_id])\n",
    "actual = train.rating\n",
    "print()\n",
    "print('###The error in Training set is: '+str(mean_absolute_error(predicted, actual)))\n",
    "predicted = result.predict([test.user_id, test.item_id])\n",
    "actual = test.rating\n",
    "print()\n",
    "print('###The error in Test set is: '+str(mean_absolute_error(predicted, actual)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
